---
title: "qntz"
date: 2023-10-12
---

## 0. Intoduction and the general rules 

+ The general rules
Specifically for pytorch-to-engine conversion there are some general rules that do now depend on the method of coversion that you chose:
1. The model has to be single-device
2. All data has to be pytorch tensors, so no numpy array 
3. The functions used indise a model have to be pytorch functions(for example, no OpenCV functions)

![general_idea](Quantization.png)


## 1. Scripting & Tracing 

- **General idea**
The tracing and scripting modules are important to understand, no matter which method of model-to-engine conversion you will chose. 
There are two ways to make the pytorch model runnable in cpp, but not be quantised and optimized for the hardware: torch.jit.script and torch.jit.trace.  
![general_idea](Quantization_tr_sc.png)
From the [official documentation](https://pytorch.org/docs/master/jit.html): 
>TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency.
It is important to understand, that TorchScript is a general module that is used for both tracing **and** scripting.

- **The difference between torch.jit.trace and torch.jit.script**:

  - **TorchScript Tracing**:

    **What it does**: Tracing takes the model and the input, runs the input throught the model and records the operations into a graph. The resulting model becomes python-independant, just like as in scripting, but the operations will be  \
    **Method specific problems**:
      + Doesn't support control flow, so if you have a logical gate, you will get a warning(the model will compile, but not correctly)
      + Captures some variables as constants
      + Captures device, for example, if we had cpu during tracing, it will be saved as a cpu specific device 
      + Does not support some pytorch functions

  - **TorchScript Scripting**:

    **What it does**: In simple terms, scripting takes the pytorch model and transofms it into a lower level representation that can be used outside of Python. It can remove Python-specific overhead, improve memory usage, and enable the use of Just-In-Time (JIT) compilation techniques for faster execution. \
    **Method specific problems**:
      + Does not support a lot of native python functions
      + Does not support a lot of pytorch functions 
      + Does not allow global variables 
      + Need type hints, otherwise assumes everything to be a PyTorch tensor
      + The types of the variables need to be static 
      + Every custom function inside also needs to be scripted
      + 
- **When to use which method**:
  THere are several different sources that help to undesrtand when to use tracing or scripting. Sadly, the do not always agree which method is the preferred one. For example, [this blogpost](https://paulbridger.com/posts/mastering-torchscript/) argues, that scripting is a good choice for the default method, because it is more beginner-frindly with easier errors. On the contrary, [this blogpost](https://ppwwyyxx.com/blog/2022/TorchScript-Tracing-vs-Scripting/) argues, that the model whould be mostly traced, because scripting demand to have very simple syntax, which can significantly impact the quality of the code. Both blogpost are very informative and compliment each other well, talking in detail about the errors the user and encounter while tracings/scripting and how to fix them. I hightly recommend them to better undertand the subject.
   
    



## 2. The pytorch model-ONNX-TensorRT

- **What is ONNX model?**\
ONNX (Open Neural Network Exchange) is a serialized representation of the model, and it can be used with various deep learning frameworks and runtime libraries that support the ONNX standard. ONNX files contain model architectures, layer configurations, and learnable parameters in a platform-independent format. ONNX is supported by a wide ecosystem of tools and libraries, enabling efficient inference on various hardware and platforms. A resulting .onnx file can be used both for using a model in another DL framework and deploying the model in TensorRT.

- **How does the pytorch.onnx function works?**\
The code to transform a PyTorch model to ONNX looks like this:

```
import torch
import torchvision.models as models
import torch.onnx as onnx

def transform_model_to_onnx(model, onnx_filename):
    # Create a pre-trained PyTorch model
    model.eval()

    # Create example input data
    input_data = torch.randn(1, 3, 224, 224)

    # Export the model to ONNX format
    torch.onnx.export(model, input_data, onnx_filename, verbose=True, opset_version=11)

if __name__ == '__main__':
    resnet_model = models.resnet18(pretrained=True)
    resnet_onnx_filename = "resnet18.onnx"
    transform_model_to_onnx(model=resnet_model, onnx_filename=resnet_onnx_filename)
    
```
The code itself is not comlicated, but it is important to understand exactly what is going on here. 
The ONNX graph is created when you export a deep learning model from a framework like PyTorch or TensorFlow. The process involves several steps:\

1. Model Tracing:
The first step in creating an ONNX graph is to trace the model using example input data. This means running the model with sample inputs to capture a sequence of operations.
Tracing is necessary to record the operations and their order as the input data flows through the model. This tracing process is framework-specific and depends on the deep learning framework being used.

2. Node Creation:
As the model is traced, each operation or layer becomes a node in the ONNX graph. These nodes represent the computations performed by the model. For example, a convolutional layer, a batch normalization layer, or an activation function each corresponds to a node in the graph.  The ONNX graph specifies the data types and shapes of tensors at each node. It also includes input and output tensors, which represent the data flowing into and out of the model.

3. Serialization Format:
The entire ONNX graph, including its structure, node attributes, input/output tensors, data types, shapes, and serialized parameters, is saved in a serialized format.
The serialization format can be binary or text-based, but it's designed to be efficient for storage and transmission.
Export to File:

The final step is to save the ONNX graph to a file. 

On the backend, the function torch.onnx uses either **scripting** or **tracing**. Also, it can take both a pytoch model, or a .pt model as input. [The official documentation](https://pytorch.org/docs/master/onnx.html) descibes it the following way:

>Internally, torch.onnx.export() requires a torch.jit.ScriptModule rather than a torch.nn.Module. If the passed-in model is not already a ScriptModule, export() will use tracing to convert it to one:
>>**Tracing**: If torch.onnx.export() is called with a Module that is not already a ScriptModule, it first does the equivalent of torch.jit.trace(), which executes the model once with the given args and records all operations that happen during that execution. This means that if your model is dynamic, e.g., changes behavior depending on input data, the exported model will not capture this dynamic behavior. We recommend examining the exported model and making sure the operators look reasonable. Tracing will unroll loops and if statements, exporting a static graph that is exactly the same as the traced run. If you want to export your model with dynamic control flow, you will need to use scripting.
>>**Scripting**: Compiling a model via scripting preserves dynamic control flow and is valid for inputs of different sizes. To use scripting:
Use torch.jit.script() to produce a ScriptModule.
Call torch.onnx.export() with the ScriptModule as the model. The args are still required, but they will be used internally only to produce example outputs, so that the types and shapes of the outputs can be captured. No tracing will be performed.

- **The limitations of ONNX**
1. Complex Control Flows
During the creation of the .onnx file the model is traced by running the dummy input thorugh it. Because of it, all the logical gates (if oprations, that are dependant on the input to the model, loops) will be frozen. If during the tracing process one option was chosen, it will be the default option in the future. This means, that your model will comply and the .onnx file will be saved, but there is a possibility it will not work as intended.
2. PyTorch function support
Not all PyTorch function are supported in the onnx format. In most cases, the issue can be solved by rewriting the function manually. 
