---
title: "qntz"
date: 2023-10-12
---

### 0. Intoduction

### 1. The general rules
Specifically for pytorch-to-engine conversion there are some general rules that do now depend on the method of coversion that you chose:
- The model has to be single-device
- All data has to be pytorch tensors, so no numpy array 
- The functions used indise a model have to be pytorch functions(for example, no OpenCV functions)

### 2. The pytorch model-ONNX-TensorRT

- **What is ONNX model?**\
ONNX (Open Neural Network Exchange) is a serialized representation of the model, and it can be used with various deep learning frameworks and runtime libraries that support the ONNX standard. ONNX files contain model architectures, layer configurations, and learnable parameters in a platform-independent format. ONNX is supported by a wide ecosystem of tools and libraries, enabling efficient inference on various hardware and platforms. A resulting .onnx file can be used both for using a model in another DL framework and deploying the model in TensorRT.

- **How does the pytorch.onnx function works?**\
The code to transform a PyTorch model to ONNX looks like this:

```
import torch
import torchvision.models as models
import torch.onnx as onnx

def transform_model_to_onnx(model, onnx_filename):
    # Create a pre-trained PyTorch model
    model.eval()

    # Create example input data
    input_data = torch.randn(1, 3, 224, 224)

    # Export the model to ONNX format
    torch.onnx.export(model, input_data, onnx_filename, verbose=True, opset_version=11)

if __name__ == '__main__':
    resnet_model = models.resnet18(pretrained=True)
    resnet_onnx_filename = "resnet18.onnx"
    transform_model_to_onnx(model=resnet_model, onnx_filename=resnet_onnx_filename)
    
```
The code itself is not comlicated, but it is important to understand exactly what is going on here. 
The ONNX graph is created when you export a deep learning model from a framework like PyTorch or TensorFlow to the ONNX format. The process involves several steps:\

1. Model Tracing:
The first step in creating an ONNX graph is to trace the model using example input data. This means running the model with sample inputs to capture a sequence of operations.
Tracing is necessary to record the operations and their order as the input data flows through the model. This tracing process is framework-specific and depends on the deep learning framework being used.

2. Node Creation:
As the model is traced, each operation or layer becomes a node in the ONNX graph. These nodes represent the computations performed by the model. For example, a convolutional layer, a batch normalization layer, or an activation function each corresponds to a node in the graph.  The ONNX graph specifies the data types and shapes of tensors at each node. It also includes input and output tensors, which represent the data flowing into and out of the model.

3. Serialization Format:
The entire ONNX graph, including its structure, node attributes, input/output tensors, data types, shapes, and serialized parameters, is saved in a serialized format.
The serialization format can be binary or text-based, but it's designed to be efficient for storage and transmission.
Export to File:

The final step is to save the ONNX graph to a file. 

- **The limitations of ONNX**\
During the creation of the .onnx file the model is trased, by running the dummy input thorugh it. Because of it, all the logical gates (if oprations, that are dependant on the input to the model) will be frozen. If during the trasing process one option was chosen, this will be the default option in the future. This means, that your model will comply and the file will be saved, but there is possibility it will not work as intended. 
